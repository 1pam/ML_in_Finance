{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "## From Regression to Classification\n",
    "\n",
    "In the previous chapter we discussed linear regression and its application. It is a very valuable tool because\n",
    "\n",
    "* it is easy to fit\n",
    "* has simple interpretation and \n",
    "* tests of statistical significance can be performed without difficulty.\n",
    "\n",
    "But it has its drawbacks too. For example: it makes quite strong assumptions about the functional form of $f(X)$ and the error terms. Another disadvantage is that linear regression is not suited for classification problems where we deal with qualitative responses. \n",
    "\n",
    "Below figure, where we regress the probability of default onto credit card balance, visualizes the issue. \n",
    "\n",
    "* Linear regression produces negative probabilities for some balances\n",
    "* Due to the large number of non-defaults, the linear regression's probability of default barely rises above 25%. This begs the question where we should set the threshold to qualify a default case. It can't be 50%, can it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Graphics/0206_LinRegClassification.png\" alt=\"LinearRegClassification\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression is able to overcame these obstacles. Rather than modeling the default class $y$ (e.g. 0 for non-default, 1 for default) as in linear regression it models the (conditional) probability that $y$ belongs to a certain category, $\\Pr(Y=y|X)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Model\n",
    "\n",
    "Above figure estimated the probability of default given a client's credit card balance. In the same way we could have used a multiple linear regression to estimate the probability. \n",
    "\n",
    "$$\\begin{equation}\n",
    "\\Pr(Y=y|X) = \\beta_0 + \\beta_1 X_1 + \\ldots + \\beta_p X_p\n",
    "\\end{equation}$$\n",
    "\n",
    "But as discussed above for the simple case, this too would yield unbounded probabilities\\footnote{For convenience let us assume that we are using the generic 0/1 coding for the dependent variable.} with $y > 1$ and $y<0$ for some $X$. The question thus is: how could we improve on that model to have bounded results with probabilities of default? A well known function which satisfies the inequality $0 \\leq f(x) \\leq 1$ (and is continuous and hence differentiable) is the so called \"Sigmoid\" or \"Logistic\" function which is defined as \n",
    "\n",
    "$$\\begin{equation}\n",
    "S(x) = \\frac{1}{1 + e^{-x}} = \\frac{e^x}{1 + e^x}\n",
    "\\end{equation}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Graphics/0206_Sigmoid.png\" alt=\"LogisticFunction\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above figure displays the function's shape. Its range is bounded by $[0, 1]$ and its notable \"S\" shape depends on the input parameter $x$. For logistic regression $x$ takes on the known functional form of a linear regression:\n",
    "\n",
    "$$\\begin{equation}\n",
    "\\Pr(Y=y|X) = \\frac{e^{\\beta_0 + \\beta_1 X_1 + \\ldots + \\beta_p X_p}}{1 + e^{\\beta_0 + \\beta_1 X_1 + \\ldots + \\beta_p X_p}}\n",
    "\\end{equation}$$\n",
    "\n",
    "This function is related to the linear model in that by rearranging the equation we arrive at\n",
    "\n",
    "$$\\begin{equation*}\n",
    "\\frac{p(X)}{1 - p(X)} = e^{\\beta_0 + \\beta_1 X_1 + \\ldots + \\beta_p X_p}\n",
    "\\end{equation*}$$\n",
    "\n",
    "Notice that for ease of use we use $p(X)$ as equivalent of $\\Pr(Y=y|X)$. Taking the natural logarithm of this expression yields\n",
    "\n",
    "$$\\begin{equation}\n",
    "\\underbrace{\\log\\left(\\frac{p(X)}{1 - p(X)}\\right)}_{\\textit{Logit}} = \\beta_0 + \\beta_1 X_1 + \\ldots + \\beta_p X_p,\n",
    "\\end{equation}$$\n",
    "\n",
    "a function where the output on the left-hand side, the so called **Logit** or **log-odds**, is linear in $X$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating the Coefficients\n",
    "\n",
    "To run a logistic regression we need to estimate the $\\beta$ coefficients in above equation. In the linear regression setting we used a least squares approach. Here, however, we switch to a method called **maximum likelihood**. This approach is used in many areas of statistics/machine learning to fit models and as such often studied in graduate courses. Here we will restrict ourselves to a very superficial discussion of the intuition. For the interested reader, [Elkan (2014)](http://cseweb.ucsd.edu/~elkan/250B/logreg.pdf) provides an excellent, concise introduction, whereas Bishop (2006) has a more advanced, yet sound discussion of the topic. \n",
    "\n",
    "Back to the intuition: maximum likelihood optimizes the Logit-equation to get estimates of $\\beta_0, \\beta_1$ \"such that the predicted probability $\\hat{p}(x_i)$ of default for each individual [$\\ldots$] corresponds as closely as possible to the individual's observed default status. In other words, we try to find $\\hat{\\beta}_0$ and $\\hat{\\beta}_1$ such that plugging these estimates into the model for $p(X)$ [$\\ldots$] yields a number close to one for all individuals who defaulted, and a number close to zero for all individuals who did not.\" (James et al. (2013, p. 133)). \n",
    "\n",
    "The maximum likelihood function that is used to estimate the $\\beta$ coefficients in the logistic regression model, is derived from the probability distribution of the dependent variable $y$. Since $y$ takes on one of two values (in our example $y \\in \\{0, 1\\}$) and assuming the response values are independent of each other, the probability mass function of $Y$ follows a Bernoulli distribution, $Y \\sim Bern(p)$, which is a special case of the Binomial distribution $Bin(n, p)$ with $n=1$. Its probability mass function $f(y;x)$ is \n",
    "\n",
    "\\begin{equation*}\n",
    "f(y;p) = p(Y=y) = p^y (1-p)^{1-y} = \n",
    "\\begin{cases}\n",
    "p & \\text{ if } y=1 \\\\\n",
    "(1-p) & \\text{ if } y=0 \n",
    "\\end{cases}\n",
    "\\end{equation*}\n",
    "\n",
    "The joint probability mass function of $Y$ is \n",
    "\n",
    "\\begin{equation*}\n",
    "f(y | \\beta) = \\prod_{i=1}^N p(x_i)^{y_i} \\, (1-p(x_i))^{1-y_i}\n",
    "\\end{equation*}\n",
    "\n",
    "and describes the values of $y$ as a function of known, fixed values for $\\beta$, where $\\beta$ is related to $y$ through the logistic function. Since we don't know the coefficients $\\beta$ but have measured outcomes for $y$ the likelihood function reverses above joint probability mass function such that it expresses the values of $\\beta$ in terms of known, fixed values for $y$. This is the likelihood function ([Czepiel (2002)](https://czep.net/stat/mlelr.pdf)).\n",
    "\n",
    "\\begin{equation}\n",
    "L(\\beta | y) = \\prod_{i=1}^N p(x_i)^{y_i} \\, (1-p(x_i))^{1-y_i}\n",
    "\\end{equation}\n",
    "\n",
    "The maximum likelihood estimates are now those values for $\\beta$ which maximize the likelihood function $L(\\beta | y)$. Typically, to find the maximum likelihood estimates we would differentiate the (log-) likelihood with respect to the coefficients, set the derivatives equal to zero, and solve. However, there is no closed-form solution for this and thus numerical methods (such as Newton-Raphson, Newton-conjugent gradient etc.) are required to derive a maxima ([Shalizi (2017)](http://www.stat.cmu.edu/~cshalizi/ADAfaEPoV/ADAfaEPoV.pdf)). Thankfully, statistical packages such as Python's `statsmodels` have these necessary tools integrated into the relevant functions such that we do not need to concern ourselves with the details. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression in Python\n",
    "\n",
    "### Package Selection\n",
    "\n",
    "To show how logistic regression is run in Python we will again rely on functions from the `statsmodels` package. The scikit-learn package `sklearn` contains similar functions. However, when it comes to calling results, the `statsmodels` functions for logistic regression follow those for linear regression which we got to know in the previous chapter. Therefore we will work with this package. \n",
    "\n",
    "### Data Load\n",
    "\n",
    "We will use the generic 'Default' data set from James et al. (2013), which we discussed in this chapter's introduction above. It is taken from the book's corresponding `R` package and made available in this course's data folder on GitHub. We start our journey as usual with the initial load of the necessary packages and setting a few options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ben Zimmermann\\Anaconda3\\lib\\site-packages\\statsmodels\\compat\\pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "plt.style.use('seaborn-whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>default</th>\n",
       "      <th>student</th>\n",
       "      <th>balance</th>\n",
       "      <th>income</th>\n",
       "      <th>defaultFac</th>\n",
       "      <th>studentFac</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>729.526495</td>\n",
       "      <td>44361.625074</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>817.180407</td>\n",
       "      <td>12106.134700</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>1073.549164</td>\n",
       "      <td>31767.138947</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>529.250605</td>\n",
       "      <td>35704.493935</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>785.655883</td>\n",
       "      <td>38463.495879</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  default student      balance        income  defaultFac  studentFac\n",
       "0      No      No   729.526495  44361.625074           0           0\n",
       "1      No     Yes   817.180407  12106.134700           0           1\n",
       "2      No      No  1073.549164  31767.138947           0           0\n",
       "3      No      No   529.250605  35704.493935           0           0\n",
       "4      No      No   785.655883  38463.495879           0           0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Default data set is not available online. Data was extracted from R package \"ISLR\"\n",
    "df = pd.read_csv('Data/Default.csv', sep=',')\n",
    "\n",
    "# Factorize 'No' and 'Yes' in columns 'default' and 'student'\n",
    "df['defaultFac'] = df.default.factorize()[0]\n",
    "df['studentFac'] = df.student.factorize()[0]\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Logistic Regression\n",
    "\n",
    "Similar to linear regression with `statsmodels` there exist different ways to compute a logistic regression. For reference, we will show three approaches here:\n",
    "\n",
    "1. R-Style: The `statsmodels.formula.api` library has a `glm` function mimicking `R`'s way of running glm regressions\n",
    "2. Classic GLM: Using the `GLM` function from the `statsmodels.api` library \n",
    "3. Logit regression: `statsmodels.api`'s `Logit` function follows the approach we used for OLS case \n",
    "\n",
    "You might ask what GLM/glm means. It stands for *Generlized Linear Models* and as such is generalization of linear regression approaches (linear, logistic, Poisson regression). There are many good resources available on the web for the interested. A good textbook introducing GLM is Alan Agresti's *Foundations of Linear and Generalized Linear Models* (Agresti (2015))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### R-Style\n",
    "\n",
    "* This method allows for a verbose definition of the regression. \n",
    "* It follows the pattern `y ~ x1 + x2 + ... + xn` - in words we regress $y$ on $x_1, x_2, \\ldots, x_n$ where $x_i$ is a vector of a particular feature. \n",
    "* By default a constant is added to the regression\n",
    "* `familiy` is a necessary argument. Remember that above where we discussed the maximum likelihood approach of estimating the coefficients we mentioned that the binary response $y$ follows a Bernoulli distribution, which is a special case of a Binomial distribution. Thus we chose `family=sm.families.Binomial()`. \n",
    "* The `.fit()` method runs the maximum likelihood estimation of the coefficients. Default method is 'Newton-Raphson' (method=`newton`) and this fits our needs. Nonetheless, others are available. Use the help function `sm.Logit.fit?` to see what options you have (it's the same for all three approaches).\n",
    "\n",
    "More details can be found [here](http://www.statsmodels.org/dev/example_formulas.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Generalized Linear Model Regression Results                  \n",
      "==============================================================================\n",
      "Dep. Variable:             defaultFac   No. Observations:                10000\n",
      "Model:                            GLM   Df Residuals:                     9998\n",
      "Model Family:                Binomial   Df Model:                            1\n",
      "Link Function:                  logit   Scale:                             1.0\n",
      "Method:                          IRLS   Log-Likelihood:                -798.23\n",
      "Date:                Sat, 02 Sep 2017   Deviance:                       1596.5\n",
      "Time:                        13:44:06   Pearson chi2:                 7.15e+03\n",
      "No. Iterations:                     9                                         \n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept    -10.6513      0.361    -29.491      0.000     -11.359      -9.943\n",
      "balance        0.0055      0.000     24.952      0.000       0.005       0.006\n",
      "==============================================================================\n"
     ]
    }
   ],
   "source": [
    "# 1. R-Style\n",
    "\n",
    "# R-Style formula\n",
    "formula = 'defaultFac ~ balance'\n",
    "\n",
    "# Regress model\n",
    "logReg = smf.glm(formula=formula, data=df, family=sm.families.Binomial()).fit()\n",
    "print(logReg.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classic GLM\n",
    "\n",
    "* `endog` = endogenous variable = response = y\n",
    "* `exog` = exogenous variable(s) = features = X\n",
    "* Notice that a constant needs to be added manually with `sm.add_constant(X)`\n",
    "\n",
    "Function details are described [here](http://www.statsmodels.org/0.6.1/generated/statsmodels.genmod.generalized_linear_model.GLM.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Generalized Linear Model Regression Results                  \n",
      "==============================================================================\n",
      "Dep. Variable:             defaultFac   No. Observations:                10000\n",
      "Model:                            GLM   Df Residuals:                     9998\n",
      "Model Family:                Binomial   Df Model:                            1\n",
      "Link Function:                  logit   Scale:                             1.0\n",
      "Method:                          IRLS   Log-Likelihood:                -798.23\n",
      "Date:                Sat, 02 Sep 2017   Deviance:                       1596.5\n",
      "Time:                        13:44:06   Pearson chi2:                 7.15e+03\n",
      "No. Iterations:                     9                                         \n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const        -10.6513      0.361    -29.491      0.000     -11.359      -9.943\n",
      "balance        0.0055      0.000     24.952      0.000       0.005       0.006\n",
      "==============================================================================\n"
     ]
    }
   ],
   "source": [
    "logReg = sm.GLM(endog=df.defaultFac, exog=sm.add_constant(df.balance), \n",
    "                family=sm.families.Binomial()).fit()\n",
    "print(logReg.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logit Regression\n",
    "\n",
    "* `endog` = endogenous variable = response = y\n",
    "* `exog` = exogenous variable(s) = features = X\n",
    "* Notice that a constant needs to be added manually with `sm.add_constant(X)`\n",
    "* No need to specify `family` (Logit is per default binomial)\n",
    " \n",
    "\n",
    "The full function details are to be found [here](http://www.statsmodels.org/dev/generated/statsmodels.discrete.discrete_model.Logit.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.079823\n",
      "         Iterations 10\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:             defaultFac   No. Observations:                10000\n",
      "Model:                          Logit   Df Residuals:                     9998\n",
      "Method:                           MLE   Df Model:                            1\n",
      "Date:                Sat, 02 Sep 2017   Pseudo R-squ.:                  0.4534\n",
      "Time:                        13:44:06   Log-Likelihood:                -798.23\n",
      "converged:                       True   LL-Null:                       -1460.3\n",
      "                                        LLR p-value:                6.233e-290\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const        -10.6513      0.361    -29.491      0.000     -11.359      -9.943\n",
      "balance        0.0055      0.000     24.952      0.000       0.005       0.006\n",
      "==============================================================================\n",
      "\n",
      "Possibly complete quasi-separation: A fraction 0.13 of observations can be\n",
      "perfectly predicted. This might indicate that there is complete\n",
      "quasi-separation. In this case some parameters will not be identified.\n"
     ]
    }
   ],
   "source": [
    "logReg = sm.Logit(endog=df.defaultFac, exog=sm.add_constant(df.balance)).fit()\n",
    "print(logReg.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accessing Output\n",
    "\n",
    "Though the scope of information in the summaries differ slightly between the different approaches the function produce the same output. As in the case of linear regression, the model's output can be accessed through its different methods and attributes. For example the models parameters and p-values are easily accessed as shown below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "const     -10.651331\n",
       "balance     0.005499\n",
       "dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logReg.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "const      3.723665e-191\n",
       "balance    2.010855e-137\n",
       "dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logReg.pvalues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothesis Testing\n",
    "\n",
    "The $z$-statistic in above summary and the corresponding p-values play the same role as the $t$-statistic in the linear regression output. For example, the $z$-statistic associated with $\\hat{\\beta}_1$ is equal to $\\hat{\\beta}_1 / SE(\\hat{\\beta}_1)$. A large (absolute) $z$-value, and correspondingly a small p-value, provides evidence against the null hypothesis $H_0 : \\beta_1 = 0$. Notice that this null hypothesis implies that $p(X) = \\dfrac{e^{\\beta_0}}{(1 + e^{\\beta_0})}$. \n",
    "\n",
    "The main purpose of the intercept is to adjust the average fitted probabilities to the proportion of ones in the data. Beyond that it is not of interest.\n",
    "\n",
    "If we are interested in the coefficient's 99% CI (instead of 95% like in the summary) we can use the same code as in the linear regression case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 0         1\n",
      "const   -11.581640 -9.721022\n",
      "balance   0.004931  0.006067\n"
     ]
    }
   ],
   "source": [
    "print(logReg.conf_int(alpha=0.01))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Predictions\n",
    "\n",
    "If we wish to make predictions of the default probability we can use the `.predict()` method. Given we leave the brackets empty, this method will calculate $p(X)$ on the basis of the previously used feature training sample. If we wish to get the probability of default for a balance of, let's say, USD 2'000 we can run the following line of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.58576937])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X must include 1 in first column for intercept\n",
    "logReg.predict([1, 2000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 2x1 vector $[1, 2000]$ in the above `.predict()` function corresponds to a row vector containing the $x$-values for which we wish to predict the probability of default. If we had $p$ features, this vector would obviously have lenght $p+1$ ($p$ features + 1 for intercept).\n",
    "\n",
    "For future reference, below code generates a confusion matrix. We will discuss Confusion Matrices in more detail in the chapter on $k$-Nearest Neighbor. The `threshold` parameter defines above what probability an new observation is labeled as 1; default is 0.5. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 9625.,    42.],\n",
       "       [  233.,   100.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Confusion Matrix for LogRegression\n",
    "logReg.pred_table(threshold=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting\n",
    "\n",
    "Above we regressed default on balance. Thus this can still be displayed in a figure. In order to do so we need to sort the data first. Once this is done, plotting follows the usual routine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAewAAAE4CAYAAACZhcBZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nHiCwAQDwAIENAIAHCGwAADxAYAMA4IEv3HbFmqKhTIkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1ab16d2e588>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create sorted resutls-df\n",
    "res = pd.DataFrame()\n",
    "res['balance'] = df.balance\n",
    "res['prob'] = logReg.predict()\n",
    "res = res.sort_values('balance')\n",
    "\n",
    "# Plot scatter & logReg\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.scatter(df.balance, df.defaultFac, marker='.')\n",
    "plt.plot(res.balance, res.prob, c='k')\n",
    "plt.axhline(y=0, color='gray', linestyle='dashed')\n",
    "plt.axhline(y=1, color='gray', linestyle='dashed')\n",
    "plt.ylabel('Probability of default', fontsize=12)\n",
    "plt.xlabel('Balance', fontsize=12);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Logistic Regression\n",
    "\n",
    "Regressing 'default' on multiple features follows the same procedure as for simple logistic regression. As a case of use we regress 'default' on 'balance', a credit card holder's 'income' and the dummy variable 'studentFac' (1 if yes, 0 otherwise). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.078577\n",
      "         Iterations 10\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const        -10.8690      0.492    -22.079      0.000     -11.834      -9.904\n",
      "balance        0.0057      0.000     24.737      0.000       0.005       0.006\n",
      "income         0.0030      0.008      0.370      0.712      -0.013       0.019\n",
      "studentFac    -0.6468      0.236     -2.738      0.006      -1.110      -0.184\n",
      "==============================================================================\n"
     ]
    }
   ],
   "source": [
    "# Assign features to matrix X and response to y\n",
    "X = sm.add_constant(df[['balance', 'income', 'studentFac']])\n",
    "X.income = X.income / 1000  # Income in 1'000 dollars\n",
    "y = df.defaultFac\n",
    "\n",
    "# Run Logistic Regression\n",
    "logReg = sm.Logit(endog=y, exog=X).fit()\n",
    "print(logReg.summary().tables[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The p-value for `income` is surprisingly large, indicatign that there is no clear evidence of a real association between the probability of default and income. In a case study this certainly would need to be further examined. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Decision Boundary\n",
    "\n",
    "We've seen above how one can plot a logistic regression in the simple case with just one feature. If you regress a response on two features, it is still possible to create very meaningful plots. A good example is [this answer on stackoverflow](https://stackoverflow.com/questions/28256058/plotting-decision-boundary-of-logistic-regression) where the decision boundary is drawn and the color scheme follows the probability of the response. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Further Ressources\n",
    "\n",
    "\n",
    "In writing this notebook, many ressources were consulted. For internet ressources links are provided within the textflow above. Beyond these links, the following ressources were consulted and are recommended as further reading on the discussed topics:\n",
    "\n",
    "* Agresti, Alan, 2015, *Foundations of Linear and Generalized Linear Models* (John Wiley & Sons, Hoboken, NJ).\n",
    "* Bishop, Christopher M., 2006, *Pattern Recognition and Machine Learning* (Springer, New York, NY).\n",
    "* Czepiel, Scott A., 2002, Maximum Likelihood Estimation of Logistic Regression Models: Theory and Implementation from website, https://czep.net/stat/mlelr.pdf, 08/24/17.\n",
    "* Elkan, Charles, 2014, Maximum Likelihood, Logistic Regression, and Stochastic Gradient Training from website, http://cseweb.ucsd.edu/~elkan/250B/logreg.pdf, 08/24/17.\n",
    "* James, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani, 2013, *An Introduction to Statistical Learning: With Applications in R* (Springer Science & Business Media, New York, NY).\n",
    "* Shalizi, Cosma Rohilla, 2017, Advanced Data Analysis from an Elementary Point of View from website, http://www.stat.cmu.edu/~cshalizi/ADAfaEPoV/ADAfaEPoV.pdf, 08/24/17.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
